README for Gabe Holmes Project5 - Web Crawler 
This project proved to be simplier than the other projects we have worked on. The high level approach is pretty simple. The crawler sends a GET request to the login page, and reads its response. The response inlcudes a csrf token that needs to be supplied along with the username and password. This inforamtion, along with the cookie provided, is supplied in a POST request to the login page. This was the most challenging part of the projet for me. I had my browser open and made many login requests in order to see exactly what my browser was sending. This information was very useful and I was able to extract the important information and create my own post request. The server then lets the crawler log in by sending it a sessionid. I then moved on to parsing the HTML for links, and adding them to the frontier if they have not already been visited. I ran into a problem trying to crawl these pages at first. I did not realize that the server will respond with a new sessionID occassionaly, and I cannot just use the one provided after logging in. Once I checked for a new sessionID after every request the crawler was able to start working. However, the first few runs I noticed the crawler would stop working after about 1000 requests. I then tried opening a new socket after every 100 requests, and this solved the issue. Finally, after some more html parsing, I was able to get the 5 flags and print them out, then exit the program. 